{"container_type": "Author", "filled": ["basics", "publications", "indices", "counts"], "scholar_id": "eUiG0O0AAAAJ", "source": "AUTHOR_PROFILE_PAGE", "name": "Junyi Ao", "url_picture": "https://scholar.googleusercontent.com/citations?view_op=view_photo&user=eUiG0O0AAAAJ&citpid=5", "affiliation": "The Chinese University of Hong Kong, Shenzhen", "interests": ["Speech Recognition", "Self-Supervised Learning"], "email_domain": "@link.cuhk.edu.cn", "homepage": "https://ajyy.github.io/", "citedby": 75, "publications": {"eUiG0O0AAAAJ:d1gkVwhDpl0C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing", "pub_year": "2022"}, "filled": false, "author_pub_id": "eUiG0O0AAAAJ:d1gkVwhDpl0C", "num_citations": 43, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=5341736616533210721", "cites_id": ["5341736616533210721"]}, "eUiG0O0AAAAJ:9yKSN-GCB0IC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "LightHuBERT: Lightweight and Configurable Speech Representation Learning with Once-for-All Hidden-Unit BERT", "pub_year": "2022"}, "filled": false, "author_pub_id": "eUiG0O0AAAAJ:9yKSN-GCB0IC", "num_citations": 11, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=5537527528769427293", "cites_id": ["5537527528769427293"]}, "eUiG0O0AAAAJ:u-x6o8ySG0sC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Multi-View Self-Attention Based Transformer for Speaker Recognition", "pub_year": "2022"}, "filled": false, "author_pub_id": "eUiG0O0AAAAJ:u-x6o8ySG0sC", "num_citations": 10, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=3535805686887714253", "cites_id": ["3535805686887714253"]}, "eUiG0O0AAAAJ:IjCSPb-OGe4C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "SpeechUT: Bridging Speech and Text with Hidden-Unit for Encoder-Decoder Based Speech-Text Pre-training", "pub_year": "2022"}, "filled": false, "author_pub_id": "eUiG0O0AAAAJ:IjCSPb-OGe4C", "num_citations": 5, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=8963519551547997493", "cites_id": ["8963519551547997493"]}, "eUiG0O0AAAAJ:2osOgNQ5qMEC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Pre-Training Transformer Decoder for End-to-End ASR Model with Unpaired Speech Data", "pub_year": "2022"}, "filled": false, "author_pub_id": "eUiG0O0AAAAJ:2osOgNQ5qMEC", "num_citations": 5, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=4320760502099413219", "cites_id": ["4320760502099413219"]}, "eUiG0O0AAAAJ:UeHWp8X0CEIC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "The YiTrans Speech Translation System for IWSLT 2022 Offline Shared Task", "pub_year": "2022"}, "filled": false, "author_pub_id": "eUiG0O0AAAAJ:UeHWp8X0CEIC", "num_citations": 1, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=11925822607497185443", "cites_id": ["11925822607497185443"]}, "eUiG0O0AAAAJ:Tyk-4Ss8FVUC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "token2vec: A Joint Self-Supervised Pre-training Framework Using Unpaired Speech and Text", "pub_year": "2022"}, "filled": false, "author_pub_id": "eUiG0O0AAAAJ:Tyk-4Ss8FVUC", "num_citations": 0}, "eUiG0O0AAAAJ:zYLM7Y9cAGgC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "CoBERT: Self-Supervised Speech Representation Learning Through Code Representation Learning", "pub_year": "2022"}, "filled": false, "author_pub_id": "eUiG0O0AAAAJ:zYLM7Y9cAGgC", "num_citations": 0}, "eUiG0O0AAAAJ:qjMakFHDy7sC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "The YiTrans End-to-End Speech Translation System for IWSLT 2022 Offline Shared Task", "pub_year": "2022"}, "filled": false, "author_pub_id": "eUiG0O0AAAAJ:qjMakFHDy7sC", "num_citations": 0}, "eUiG0O0AAAAJ:u5HHmVD_uO8C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Improving Attention-based End-to-end ASR by Incorporating an N-gram Neural Network", "pub_year": "2021"}, "filled": false, "author_pub_id": "eUiG0O0AAAAJ:u5HHmVD_uO8C", "num_citations": 0}}, "citedby5y": 75, "hindex": 5, "hindex5y": 5, "i10index": 3, "i10index5y": 3, "cites_per_year": {"2021": 2, "2022": 65, "2023": 8}, "updated": "2023-02-12 08:18:12.075295"}